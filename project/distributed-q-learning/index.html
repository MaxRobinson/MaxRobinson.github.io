<!doctype html><html lang=en-us><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1"><meta http-equiv=x-ua-compatible content="IE=edge"><meta name=generator content="Source Themes Academia 4.3.1"><meta name=generator content="Hugo 0.101.0"><meta name=author content="Max Robinson"><meta name=description content="DistQL is a parallel, distributed, reinforcement learning system."><link rel=alternate hreflang=en-us href=https://maxrobinsoncs.com/project/distributed-q-learning/><meta name=theme-color content="#fc6f5c"><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/academicons/1.8.6/css/academicons.min.css integrity="sha256-uFVgMKfistnJAfoCUQigIl+JfUaP47GrRKjf6CTPVmw=" crossorigin=anonymous><link rel=stylesheet href=https://use.fontawesome.com/releases/v5.6.0/css/all.css integrity=sha384-aOkxzJ5uQz7WBObEZcHvV5JvRW3TUc2rNPA7pe3AwnsUohiw1Vj2Rgx2KSOkF5+h crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.css integrity="sha256-ygkqlh3CYSUri3LhQxzdcm0n1EQvH2Y+U5S2idbLtxs=" crossorigin=anonymous><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/github.min.css crossorigin=anonymous title=hl-light><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/styles/dracula.min.css crossorigin=anonymous title=hl-dark disabled><link rel=stylesheet href=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.css integrity="sha512-M2wvCLH6DSRazYeZRIm1JnYyh22purTM+FDB5CsyxtQJYeKq83arPe5wgbNmcFXGqiSH2XR8dT/fJISVA1r/zQ==" crossorigin=anonymous><link rel=stylesheet href="https://fonts.googleapis.com/css?family=Lato:400,700|Open+Sans|Roboto+Mono&display=swap"><link rel=stylesheet href=/css/academia.min.00f1f1ba4ed6dd075f62e0c0967748d4.css><script async src="https://www.googletagmanager.com/gtag/js?id=G-KSFNXZJ6QF"></script>
<script>window.dataLayer=window.dataLayer||[];function gtag(){dataLayer.push(arguments)}gtag("js",new Date),gtag("config","G-KSFNXZJ6QF")</script><script async src=https://cdnjs.cloudflare.com/ajax/libs/autotrack/2.4.1/autotrack.js integrity="sha512-HUmooslVKj4m6OBu0OgzjXXr+QuFYy/k7eLI5jdeEy/F4RSgMn6XRWRGkFi5IFaFgy7uFTkegp3Z0XnJf3Jq+g==" crossorigin=anonymous></script>
<link rel=manifest href=/site.webmanifest><link rel=icon type=image/png href=/img/logo.png><link rel=apple-touch-icon type=image/png href=/img/logo.png><link rel=canonical href=https://maxrobinsoncs.com/project/distributed-q-learning/><meta property="twitter:card" content="summary_large_image"><meta property="og:site_name" content="Max Robinson"><meta property="og:url" content="https://maxrobinsoncs.com/project/distributed-q-learning/"><meta property="og:title" content="Distributed Q-Learning | Max Robinson"><meta property="og:description" content="DistQL is a parallel, distributed, reinforcement learning system."><meta property="og:image" content="https://maxrobinsoncs.com/project/distributed-q-learning/featured.gif"><meta property="twitter:image" content="https://maxrobinsoncs.com/project/distributed-q-learning/featured.gif"><meta property="og:locale" content="en-us"><meta property="article:published_time" content="2022-06-17T00:00:00+00:00"><meta property="article:modified_time" content="2022-06-17T00:00:00+00:00"><title>Distributed Q-Learning | Max Robinson</title></head><body id=top data-spy=scroll data-target=#TableOfContents data-offset=71><aside class=search-results id=search><div class=container><section class=search-header><div class="row no-gutters justify-content-between mb-3"><div class=col-6><h1>Search</h1></div><div class="col-6 col-search-close"><a class=js-search href=#><i class="fas fa-times-circle text-muted" aria-hidden=true></i></a></div></div><div id=search-box><input name=q id=search-query placeholder=Search... autocapitalize=off autocomplete=off autocorrect=off role=textbox spellcheck=false type=search></div></section><section class=section-search-results><div id=search-hits></div></section></div></aside><nav class="navbar navbar-light fixed-top navbar-expand-lg py-0" id=navbar-main><div class=container><a class=navbar-brand href=/>Max Robinson</a>
<button type=button class=navbar-toggler data-toggle=collapse data-target=#navbar aria-controls=navbar aria-expanded=false aria-label="Toggle navigation"><span><i class="fas fa-bars"></i></span></button><div class="collapse navbar-collapse" id=navbar><ul class="navbar-nav ml-auto"><li class=nav-item><a class=nav-link href=/#about><span>Home</span></a></li><li class=nav-item><a class=nav-link href=/#projects><span>Projects</span></a></li><li class=nav-item><a class=nav-link href=/#posts><span>Posts</span></a></li><li class=nav-item><a class="nav-link js-search" href=#><i class="fas fa-search" aria-hidden=true></i></a></li><li class=nav-item><a class="nav-link js-dark-toggle" href=#><i class="fas fa-moon" aria-hidden=true></i></a></li></ul></div></div></nav><article class="article article-project py-5" itemscope itemtype=http://schema.org/Article><div class="container split-header"><div class="row justify-content-center"><div class=col-lg-8><img class="img-fluid w-100" src=/project/distributed-q-learning/featured_hueea03d360c78a434390b6e56ace269b0_679171_680x500_fill_q90_box_center.gif itemprop=image alt>
<span class=article-header-caption>Photo from gymlibrary.ml</span></div><div class=col-lg-8><h1 itemprop=name>Distributed Q-Learning</h1><meta content="2022-06-17 00:00:00 +0000 UTC" itemprop=datePublished><meta content="2022-06-17 00:00:00 +0000 UTC" itemprop=dateModified><div class=article-metadata><div><span itemprop="author name" itemtype=http://schema.org/Person><a href=/authors/max/>Max Robinson</a></span></div><span class=article-date><time>Jun 17, 2022</time></span><div class=share-box aria-hidden=true><ul class=share><li><a href="https://twitter.com/intent/tweet?url=https://maxrobinsoncs.com/project/distributed-q-learning/&text=Distributed%20Q-Learning" target=_blank rel=noopener class=share-btn-twitter><i class="fab fa-twitter"></i></a></li><li><a href="https://www.facebook.com/sharer.php?u=https://maxrobinsoncs.com/project/distributed-q-learning/&t=Distributed%20Q-Learning" target=_blank rel=noopener class=share-btn-facebook><i class="fab fa-facebook-f"></i></a></li><li><a href="mailto:?subject=Distributed%20Q-Learning&body=https://maxrobinsoncs.com/project/distributed-q-learning/" target=_blank rel=noopener class=share-btn-email><i class="fas fa-envelope"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?url=https://maxrobinsoncs.com/project/distributed-q-learning/&title=Distributed%20Q-Learning" target=_blank rel=noopener class=share-btn-linkedin><i class="fab fa-linkedin-in"></i></a></li><li><a href="https://reddit.com/submit?url=https://maxrobinsoncs.com/project/distributed-q-learning/&title=Distributed%20Q-Learning" target=_blank rel=noopener class=share-btn-reddit><i class="fab fa-reddit-alien"></i></a></li></ul></div></div><div class="btn-links mb-3"><a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/MaxRobinson/DistributedQMemory/blob/master/DistQL-Paper/DistributedQLearning.pdf target=_blank rel=noopener>PDF</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/MaxRobinson/DistributedQMemory/tree/master/DistQL target=_blank rel=noopener>Code</a>
<a class="btn btn-outline-primary my-1 mr-1" href=https://github.com/MaxRobinson/DistributedQMemory target=_blank rel=noopener><i class="fab fa-github mr-1"></i>
Check it out</a></div></div></div></div></div><div class=article-container><div class=article-style itemprop=articleBody><h1 id=distribued-q-learning>Distribued Q-Learning</h1><p>Done as part of my masters degree, this project was inspired by recent advancements in speeding up the training of reinforcment learning algorithms on deep neural networks
like what <a href=https://arxiv.org/abs/1507.04296>Gorila</a> and <a href=https://arxiv.org/abs/1602.01783>A3C</a>.
Instead of opperating on deep neural networks though, I was interested in understanding if the same types of strategies could be applied
to the traditional Q-Learning algorithm developed by Watkins.</p><p>In order to train in parrallel, the experiences from the different agents must be combined together some how. In <em>Gorila</em> and <em>A3C</em> the network
weights are combined. For this project, I had to develop my own combination equation for state-action Q-values.</p><h2 id=abstract>Abstract</h2><p>Reinforcement learning can take single agents many sequential episodes in order to
learn. To decrease the number of episodes a single agent must complete to attain a desired
performance, researchers have looked to parallel learning architectures. DistQL is a parallel,
distributed, reinforcement learning system. The system uses multiple agent-environment
pairs, where agents can learn in parallel to each other and update a central QServer. DistQL
was applied to two environments. The results showed that it is possible to have a large
decrease in the number of episodes need for an agent to perform well compared to a single
agent, as the number of distributed agents increases.</p><p>The rest of the paper can be found <a href=https://github.com/MaxRobinson/DistributedQMemory/blob/master/DistQL-Paper/DistributedQLearning.pdf>here</a> or via the link above.</p><h2 id=brief-results>Brief Results</h2><p>The results of the experiments provide insight into the performance of sets of agents when
compared to each other in the same environment. In addition, the results show how the
update methods for DistQL and the update frequency can effect learning performance. The
first environment examined is the Taxi World.</p><p>The below figures compares the performance of each set of 1, 2, 4, and 8 agents to each other using
DistQL-ALL, based on average cumulative reward compared to DistQL-Partial. Both figures shows that as the number of
agents increased, fewer episodes were require for each agent to reach a higher cumulative
reward. The figures also illustrates how much faster the set of 8 agents was able to converge
to an optimal policy when compared to the other sets of agents.</p><table><thead><tr><th style=text-align:center>DistQL-ALL Average Cumulative in Taxi world with Tau = 10</th><th style=text-align:center>DistQL-Partial Average Cumulative in Taxi world with Tau = 10</th></tr></thead><tbody><tr><td style=text-align:center><img src=binned-Average-Performance-and-Standard-Error-with-DistQL-ALL-tau-10-in-Taxi-v2.png alt></td><td style=text-align:center><img src=binned-Average-Performance-and-Standard-Error-with-DistQL-ALL-tau-10-in-Taxi-v2.png alt></td></tr></tbody></table><p>Comparing DistQL-Partial to DistQL-ALL, the figures show there is still an increase in the rate of performance as the number of agents per
set increases, but it is less pronounced. The agents all take a little longer to learn in the
beginning and then start to differentiate after that, until convergence. This suggests that
the first part of the learning process is each agent exploring. After more and more states
are discovered, the aggregation of the value of the explored states helps to give the sets
with more agents a boost in learning. This suggests that a large part of the success of
DistQL-ALL is due to the shared exploration in combination with the aggregated Q-values.</p></div><div class=article-tags><a class="badge badge-light" href=/tags/reinforcement-learning/>Reinforcement Learning</a>
<a class="badge badge-light" href=/tags/paper/>Paper</a></div><div class="media author-card" itemscope itemtype=http://schema.org/Person><div class=media-body><h5 class=card-title itemprop=name><a href=https://maxrobinsoncs.com/>Max Robinson</a></h5><h6 class=card-subtitle>Software Engineer</h6><p class=card-text itemprop=description>A self described AI enthusiast with a software engineering mindset.</p><ul class=network-icon aria-hidden=true><li><a itemprop=sameAs href=https://www.linkedin.com/in/maxrobinson31415/ target=_blank rel=noopener><i class="fab fa-linkedin"></i></a></li><li><a itemprop=sameAs href=https://github.com/MaxRobinson target=_blank rel=noopener><i class="fab fa-github"></i></a></li></ul></div></div></div></div></div></article><script src=https://cdnjs.cloudflare.com/ajax/libs/jquery/3.4.1/jquery.min.js integrity="sha256-CSXorXvZcTkaix6Yvo6HppcZGetbYMGWSFlBw8HfCJo=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/fancybox/3.2.5/jquery.fancybox.min.js integrity="sha256-X5PoE3KU5l+JcX+w09p/wHl9AzK333C4hJ2I9S5mD4M=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/highlight.js/9.15.6/highlight.min.js integrity="sha256-aYTdUrn6Ow1DDgh5JTc3aDGnnju48y/1c8s1dgkYPQ8=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.2.0/leaflet.js integrity="sha512-lInM/apFSqyy1o6s89K4iQUKg6ppXEgsVxT35HbzUupEVRh2Eu9Wdl4tHj7dZO0s1uvplcYGmt3498TtHq+log==" crossorigin=anonymous></script>
<script>hljs.initHighlightingOnLoad()</script><script>const search_index_filename="/index.json",i18n={placeholder:"Search...",results:"results found",no_results:"No results found"},content_type={post:"Posts",project:"Projects",publication:"Publications",talk:"Talks"}</script><script id=search-hit-fuse-template type=text/x-template>
      <div class="search-hit" id="summary-{{key}}">
      <div class="search-hit-content">
        <div class="search-hit-name">
          <a href="{{relpermalink}}">{{title}}</a>
          <div class="article-metadata search-hit-type">{{type}}</div>
          <p class="search-hit-description">{{snippet}}</p>
        </div>
      </div>
      </div>
    </script><script src=https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin=anonymous></script>
<script src=https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin=anonymous></script>
<script src=/js/academia.min.96c8d500634c2f327bfec9d14f53db75.js></script><div class=container><footer class=site-footer><div class=container><div class="row align-items-center"><div class="col-md-6 mb-4 mb-md-0"><p class=mb-0>Copyright Â© 2022 &#183;
Built with Hugo -
Theme derived from
<a href=https://github.com/themefisher/academia-hugo target=_blank rel=noopener>Academia Hugo</a></p></div><div class=col-md-6><ul class="list-inline network-icon text-right mb-0"><li class=list-inline-item><a href=https://github.com/MaxRobinson target=_blank rel=noopener title="Find Me"><i class="fab fa-github" aria-hidden=true></i></a></li></ul></div></div></div></footer></div><div id=modal class="modal fade" role=dialog><div class=modal-dialog><div class=modal-content><div class=modal-header><h5 class=modal-title>Cite</h5><button type=button class=close data-dismiss=modal aria-label=Close>
<span aria-hidden=true>&#215;</span></button></div><div class=modal-body><pre><code class="tex hljs"></code></pre></div><div class=modal-footer><a class="btn btn-outline-primary my-1 js-copy-cite" href=# target=_blank><i class="fas fa-copy"></i> Copy</a>
<a class="btn btn-outline-primary my-1 js-download-cite" href=# target=_blank><i class="fas fa-download"></i> Download</a><div id=modal-error></div></div></div></div></div></body></html>