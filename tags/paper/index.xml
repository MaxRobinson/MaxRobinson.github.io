<?xml version="1.0" encoding="utf-8" standalone="yes"?><rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom"><channel><title>Paper on Max Robinson</title><link>https://maxrobinsoncs.com/tags/paper/</link><description>Recent content in Paper on Max Robinson</description><generator>Source Themes academia (https://sourcethemes.com/academic/)</generator><language>en-us</language><copyright>Copyright &amp;copy; {year}</copyright><lastBuildDate>Fri, 17 Jun 2022 00:00:00 +0000</lastBuildDate><atom:link href="https://maxrobinsoncs.com/tags/paper/index.xml" rel="self" type="application/rss+xml"/><item><title>Distributed Q-Learning</title><link>https://maxrobinsoncs.com/project/distributed-q-learning/</link><pubDate>Fri, 17 Jun 2022 00:00:00 +0000</pubDate><guid>https://maxrobinsoncs.com/project/distributed-q-learning/</guid><description>&lt;h1 id="distribued-q-learning">Distribued Q-Learning&lt;/h1>
&lt;p>Done as part of my masters degree, this project was inspired by recent advancements in speeding up the training of reinforcment learning algorithms on deep neural networks
like what &lt;a href="https://arxiv.org/abs/1507.04296">Gorila&lt;/a> and &lt;a href="https://arxiv.org/abs/1602.01783">A3C&lt;/a>.
Instead of opperating on deep neural networks though, I was interested in understanding if the same types of strategies could be applied
to the traditional Q-Learning algorithm developed by Watkins.&lt;/p>
&lt;p>In order to train in parrallel, the experiences from the different agents must be combined together some how. In &lt;em>Gorila&lt;/em> and &lt;em>A3C&lt;/em> the network
weights are combined. For this project, I had to develop my own combination equation for state-action Q-values.&lt;/p>
&lt;h2 id="abstract">Abstract&lt;/h2>
&lt;p>Reinforcement learning can take single agents many sequential episodes in order to
learn. To decrease the number of episodes a single agent must complete to attain a desired
performance, researchers have looked to parallel learning architectures. DistQL is a parallel,
distributed, reinforcement learning system. The system uses multiple agent-environment
pairs, where agents can learn in parallel to each other and update a central QServer. DistQL
was applied to two environments. The results showed that it is possible to have a large
decrease in the number of episodes need for an agent to perform well compared to a single
agent, as the number of distributed agents increases.&lt;/p>
&lt;p>The rest of the paper can be found &lt;a href="https://github.com/MaxRobinson/DistributedQMemory/blob/master/DistQL-Paper/DistributedQLearning.pdf">here&lt;/a> or via the link above.&lt;/p>
&lt;h2 id="brief-results">Brief Results&lt;/h2>
&lt;p>The results of the experiments provide insight into the performance of sets of agents when
compared to each other in the same environment. In addition, the results show how the
update methods for DistQL and the update frequency can effect learning performance. The
first environment examined is the Taxi World.&lt;/p>
&lt;p>The below figures compares the performance of each set of 1, 2, 4, and 8 agents to each other using
DistQL-ALL, based on average cumulative reward compared to DistQL-Partial. Both figures shows that as the number of
agents increased, fewer episodes were require for each agent to reach a higher cumulative
reward. The figures also illustrates how much faster the set of 8 agents was able to converge
to an optimal policy when compared to the other sets of agents.&lt;/p>
&lt;table>
&lt;thead>
&lt;tr>
&lt;th style="text-align:center">DistQL-ALL Average Cumulative in Taxi world with Tau = 10&lt;/th>
&lt;th style="text-align:center">DistQL-Partial Average Cumulative in Taxi world with Tau = 10&lt;/th>
&lt;/tr>
&lt;/thead>
&lt;tbody>
&lt;tr>
&lt;td style="text-align:center">&lt;img src="binned-Average-Performance-and-Standard-Error-with-DistQL-ALL-tau-10-in-Taxi-v2.png" alt="">&lt;/td>
&lt;td style="text-align:center">&lt;img src="binned-Average-Performance-and-Standard-Error-with-DistQL-ALL-tau-10-in-Taxi-v2.png" alt="">&lt;/td>
&lt;/tr>
&lt;/tbody>
&lt;/table>
&lt;p>Comparing DistQL-Partial to DistQL-ALL, the figures show there is still an increase in the rate of performance as the number of agents per
set increases, but it is less pronounced. The agents all take a little longer to learn in the
beginning and then start to differentiate after that, until convergence. This suggests that
the first part of the learning process is each agent exploring. After more and more states
are discovered, the aggregation of the value of the explored states helps to give the sets
with more agents a boost in learning. This suggests that a large part of the success of
DistQL-ALL is due to the shared exploration in combination with the aggregated Q-values.&lt;/p></description></item></channel></rss>